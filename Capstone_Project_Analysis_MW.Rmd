---
title: "Capstone Price Comparison Project - Code Analysis"
author: "Ming Waters"
date: "April 26, 2017"
output: "html_document"
--- 

```{r, echo=FALSE, message=FALSE}
# Data Wrangling for Amazon Price versus In-Store Project # Ming Waters

library(dplyr)
library(tidyr)
library(ggplot2)
library(outliers)

```
###  Introduction and Project Goal:

In searching for a Capstone Project, I wanted to find a data set that would answer some common every day questions that will provide value for broad audience.  I also wanted a data set that would expose me to a variety of data wrangling requirements. My goal for this project is to develop my R coding skills and use R to perform statistical analysis to answer some useful every day questions. 

After exploring various open source data sets, I settled on an opens source price comparison set collected by Alberto Cavallo, co-founder of MIT’s Billion Price Project. Cavallo and team, developed a study to compare thousands of item prices on-line and in store across 10 countries. His study concluded that there were insignificant price differences between on-line and in-store prices.  His report did not compare prices by category or if there were price difference when the item was on sale. My project will use a subset of his data, US comparison, to determine if there is a particular type of product, e.g., electronics or office products, that provides better pricing when purchasing on-line or in-store and determine if there is a better strategy to purchasing when the items are on sale. [^1]


#### Project Goal and Benefit:

This project will utilize Cavallo’s price data to determine if there are significant price differences between in-store and online prices for items by category and if there is are any significant differences in prices when the item is on sale.  This analysis will benefit many shoppers streamline their choices as they shop for items by category.

#### Data Set:  

My project will wrangle and analyze a small subset of Cavallo’s multi-country data. I will focus on the US price comparisons only comparing the in store prices with the Amazon price.  

Raw Data file was downloaded from: 
[Price Data](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FXXOUHF) 

```{r, echo=FALSE, message=FALSE }
#Change working directory
setwd("C:/Users/IAZ867/Desktop/Desktop/Class/Assignments")
 
# Load original csv file and rename to w_price

 w_price <- read.csv("original_amazon_compare.csv")
```

#### Data Exploration:

After loading the files, I examined the format and the context of each variables to determine which columns contained the needed information to perform my analysis.

```{r, echo=FALSE, message=FALSE}
# Inspect Data Set and Structure
glimpse(w_price)
names(w_price)
```

Initial analysis of the raw data shows the variables and observation in the format needed to manage with R, variables are in columns.  To perform my analysis, I will need the following variables:

* price – price in store
* id – item number
* price_amazon – price on amazon
* sale_online – the item was on sale online
* PRICETYPE – if the item was on sale in store
* category – groupings of product type (electronics, office supply, etc.)

My analysis will rely on the price differences between price in-store and price on-line so I need to calculate a new field called, "delta."  

```{r, echo=FALSE, message=FALSE}
# Select needed columns 
 
w_price <- select(w_price, id, price, price_amazon, sale_online, 
                  datediff, PRICETYPE, category)

# Add new column to calculate % price difference between store price 

w_price <- mutate(w_price, "delta" = (price_amazon - price))
```

After inspecting the deltas, I realized that the price deltas were misleading since deltas for high cost values were more significant than deltas for low cost values. Most customers will value saving $1 for a $2 item as much as saving $10 for $100 item, therefore, I decided to create another field p difference, to weigh high price deltas equal to low price deltas.  

```{r}
w_price <- mutate(w_price, "p_difference" = (delta/price))
```
Now that I have created the needed variables, I inspect the new table to see how to set up my analysis.  Upon inspection I see a obvious error in price column.  On row 1614, the id number matches the price in-store.  The price is significantly higher than the on-line.  I need to remove these errors if I want to perform a good analysis.  

I create a vector that will capture the rows that have matching id and price values.  I think inspect the vector to determine how many rows I will be removing as a final check.   

```{r, echo=FALSE, message=FALSE}
# Create a vector to determine if id transfered over to price
id_typo <- which(w_price$id == w_price$price)

# View Vector Summary to determine if I will lose a significant portion of data with the removal of the typos.

str(id_typo)

```

Only two rows contain the error so I will remove the data errors.

```{r, echo=FALSE, message=FALSE}
# two rows contain match 1034, 1614, remove the rows
w_price <- w_price[-id_typo, ]

```

#### Data Entry Errors and Outliers:

Upon further inspection, it was evident that there were several large errors embedded in the data.  Data errors include –

*	Product ID transferring into In-Store Prices 
*	Errors in decimal placement  - ex – 6.99 (in-store) but typed as 69.9 (amazon)
*	Errors in data entry – ex – Door has online price 2,130 and in-store price of 3.97
*	There were subtle errors where some items did appear to be erroneous but differences and product type were unclear since Amazon may charge higher prices for small items to compensate for shipping and handling

The data set contained over 3900 rows so manual inspection of each item was not feasible.  

I decided to use ggplots to visualize thee data by categories to start my decisions around what delta ranges have gross errors.

```{r, echo=FALSE, message=FALSE}
# Graph price difference to visualize data entry errors
# determining what setting to set threshold for exclusion
ggplot(w_price, aes(x = category, y = p_difference, col = category)) + geom_point()
```

Inspection of the plot shows reasonable clustering around the difference value under 100.  If I remove all values exceeding 100, I can remove the majority of my obvious data entry errors.

```{r, message=FALSE, echo=FALSE}

# Create vector with values < 100 p_difference naming vector error
error <- which(w_price$p_difference > 100)

# Examine error vector to determine how many data points were removed
str(error)

# Remove values from dataset
w_price2 <- w_price[-error,]
```
The results shows 16 values needing removal.  After removing the values, I plot the data again by category to determine if the outliers were removed and to view the new distribution of percent differences

```{r, echo=FALSE, message=FALSE}
# Plot new data set to see range of category and verify erronous have been removed
ggplot(w_price2, aes(x = category, y = p_difference, col = category)) + geom_point()

```

The plot shows that there are still a few extreme outliers that look to be outside the normal range.  

To begin my analysis for distribution and outliers, I need to split the data frame by category. 

```{r, message=FALSE}
# Create data frames by category type
electronics_set <- filter(w_price2, category == "Electronics")
home_app <- filter(w_price2, category == "Home and Appliances")
mix <- filter(w_price2, category == "Mix")
office <- filter(w_price2, category =="Office Products")
pharm_health <- filter(w_price2, category == "Pharmacy and Health")
```

This split the data set into 5 data frames:

* Electronics Set "electronics_set"
* Home Appliances "home_app"
* Mix of Items "mix"
* Office Products "office"
* Pharmacy and Health Items "pharm_health"

I want to view the distributions of each of the categories to visually inspect normality and calculate the mean.

```{r, echo=FALSE, message=FALSE}

# Plot histogram to view distribution across categories and to determine spread
ggplot(electronics_set, aes(x = p_difference)) + geom_histogram(binwidth = .5) +
  ggtitle("Electronics") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

# Electronic sets has a skewed negative bias with a few outliers that need to be 
# investigated on the high positive side
summary(electronics_set$p_difference)

```

I repeat this exercise for each of the categories.  

```{r, echo=FALSE, message=FALSE}
#median = -0.04 data is shows amazon price bias
ggplot(home_app, aes(x = p_difference)) + geom_histogram(binwidth = 1.0) +
ggtitle("Home Appliances") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

#home appliances has extreme values on positive side and a median with zero
summary(home_app$p_difference)

# Mix category
ggplot(mix, aes(x = p_difference)) + geom_histogram(binwidth = 0.5)+
ggtitle("Mix") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))
# data shows positive skewed with extreme values
summary(mix$p_difference, na.rm = TRUE)

# Office Category

ggplot(office, aes(x = p_difference)) + geom_histogram(binwidth = 0.5) +
  ggtitle("Office") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))
# office shows small counts of positive extreme values
summary(office$p_difference)
# office shows amazon price value -0.111

# Pharmacy and Health
ggplot(pharm_health, aes(x = p_difference)) + geom_histogram(binwidth = 0.5)+ 
  ggtitle("Pharmacy and Health") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))
# data shows multiple counts of postitive extremes with large postive skew
summary(pharm_health$p_difference)
```

The histograms for all the categories do not appear to follow a normal distribution.  In addition, each category has some extreme outliers. I performed a quick Sharpiro-Wilk Normality test just to validate.  

```{r, echo=FALSE, message=FALSE}

# The data does not appear to be normally distributed for any of the categories 
# but I perform an Shapiro-Wilk normality test

shapiro.test(electronics_set$p_difference)
shapiro.test(home_app$p_difference)
shapiro.test(mix$p_difference)
shapiro.test(office$p_difference)
shapiro.test(pharm_health$p_difference)

```

Clearly the p-value shows that we can reject the null hypothesis that the data is normal.

Since the data is non-normal, I need to find a method to remove outliers.  Traditional tests such as Grubbs or Chi square test will not apply.  I can use my data set's interquartile (IQR) range to determine points as ouliers.  Removing outliers beyond 1.5*IQR will be a good baseline.  I also need to verify the calculated ranges to ensure they make "practical" sense as outliers.       

```{r, echo=FALSE, message=FALSE}
# Apply IQR to electronic data set
eI <- 1.5*(IQR(electronics_set$p_difference))

# Get quartiles
summary(electronics_set$p_difference)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -0.98920 -0.23610 -0.04924  0.40260  0.00000 79.08000 

# Calculate upper and lower thresholds
e_upper <- (0+eI)
e_lower <- (-0.23 - eI)
# lower = -0.584, upper = 0.354 
# inspect data to determine if they are valid outliers
# setting lower boundaries at < - 0.590  due to the price at -0.58 seem like a 
# reasonable price difference due to the sale notation 
# on the upper side removing outliers > 0.354 seems reasonable 
# as the price gaps between items seem questionable and no items 
# were on sale and price descriptions in original file indicates this 
# is a good threshold

# Create vector to remove outliers
out_ele <- which(electronics_set$p_difference < -0.590 
                 | electronics_set$p_difference > 0.354)
 
# Remove from set
electronics_set <- electronics_set[-out_ele,]
# Boxplot to look at spread, data is not normal but errors have been removed
boxplot(electronics_set$p_difference) 

# Repeat for home_app
# Apply IQR to electronic data set
hI <- 1.5*(IQR(home_app$p_difference))

# IQR*1.5 = 0.3377

# Get quartiles
summary(home_app$p_difference)
#      Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -0.9999 -0.1000  0.0000  1.4930  0.1252 89.7900

# Calculate upper and lower thresholds
h_upper <- (0.1252 + hI)
h_lower <- (-0.1 - hI)

# lower = -0.4377, upper = 0.46293 
# inspect data and product type to determine if they are valid outliers

# Create vector to remove outliers
out_app <- which(home_app$p_difference < -0.-4378 | home_app$p_difference > 0.462)

# Remove from set
home_app <- home_app[-out_app,]

# Boxplot to look at spread, data is not normal but errors have been removed
boxplot(home_app$p_difference)

# Reoeat for mix data
# Apply IQR to mix data set
mI <- 1.5*(IQR(mix$p_difference))

# Get quartiles
summary(mix$p_difference)
#  Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -0.97580 -0.09099  0.16520  1.57000  1.50200 51.51000 

# Calculate upper and lower thresholds
m_upper <- (1.50200 + mI)
m_lower <- (-0.09099 - mI)

# lower = -2.4801, upper = 3.891176
# inspect data to determine if they are valid outliers
# Data riddled with typos....for example cost of chapstic in store 
# 1.98 versus 253.00 on amazon, 
# inspection showed suspect errors for differenes > 2


# Create vector to remove outliers
out_mix <- which(mix$p_difference < -2.48 | mix$p_difference > 2.0)

# Remove from set
mix <- mix[-out_mix,]

# Boxplot to look at spread, data is not normal but errors have been removed
boxplot(mix$p_difference)

# Repeat for office data
# Apply IQR to office data set
oI <- 1.5*(IQR(office$p_difference))

# Get quartiles
summary(office$p_difference)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -0.9600 -0.3433 -0.1112  0.5264  0.5104 14.2400 

# Calculate upper and lower thresholds
o_upper <- (0.5104 + oI)
o_lower <- (-0.3433 - oI)

# lower = -1.6239, upper = 1.791
# inspect data to determine if they are valid outliers
# Data showed multiple entries of same item and small items like amazon cables


# Create vector to remove outliers
out_office <- which(office$p_difference < -1.6239 | office$p_difference > 1.791)

# Remove from set
office <- office[-out_office,]

# Boxplot to look at spread, data is not normal but errors have been removed
boxplot(office$p_difference)

# Repeat for Pharm_health data
# Apply IQR to office data set
pI <- 1.5*(IQR(pharm_health$p_difference))

# Get quartiles
summary(pharm_health$p_difference)
#  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -0.8004 -0.1328  2.0250  8.0040  8.8560 80.7400 

# Calculate upper and lower thresholds
p_upper <- (8.8560 + pI)
p_lower <- (-0.1328 - pI)

# lower = -13.61622, upper = 22.3394
# inspect data to determine if they are valid outliers
# Data showed multiple lots of odd high pricing for amazon items that 
# shows very low prices in store although the upper recommends 22, 
# inspetion of the data
# shows threshold of 17 on higher end is more appropriate


# Create vector to remove outliers
out_pharm <- which(pharm_health$p_difference < -13.62 | 
                     pharm_health$p_difference > 17)

# Remove from set
pharm_health <- pharm_health[-out_pharm,]

```

Now that I removed outliers from each set, I plot histograms of each of the categories to inspect the distributions of each category.

```{r, echo=FALSE, message=FALSE}

# Analysis to determine which category has the best pricing for on-line versus
# In-Store - using histograms and summary to look at over all median's 
# and distributions

# Electronics
ggplot(electronics_set, aes(x = p_difference)) + geom_histogram(binwidth = 0.1)+
  stat_bin(binwidth= .1) + ylim(c(0, 400)) +  
  stat_bin(binwidth= .1, geom="text", aes(label=..count..), vjust=-1.5) + 
  ggtitle("Percent Difference Distribution of Electronics") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

# Plot of home appliances

ggplot(home_app, aes(x = p_difference)) + geom_histogram(binwidth = 0.1)+
  stat_bin(binwidth= .1) + ylim(c(0, 200)) +  
  stat_bin(binwidth= .1, geom="text", aes(label=..count..), vjust=-1.5) + 
  ggtitle("Percent Difference Distribution of Home Appliances") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

# Plot of Mix Items

ggplot(mix, aes(x = p_difference)) + geom_histogram(binwidth = 0.1)+
  stat_bin(binwidth= .1) + ylim(c(0, 300)) +  
  stat_bin(binwidth= .1, geom="text", aes(label=..count..), vjust=-1.5) + 
  ggtitle("Percent Difference Distribution of Mix Items") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

# Plot of Office Products

ggplot(office, aes(x = p_difference)) + geom_histogram(binwidth = 0.1)+
  stat_bin(binwidth= .1) + ylim(c(0, 100)) +  
  stat_bin(binwidth= .1, geom="text", aes(label=..count..), vjust=-1.5) + 
  ggtitle("Percent Difference Distribution of Office") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))

# Plot of Pharmacy and Health Products

ggplot(pharm_health, aes(x = p_difference)) + geom_histogram(binwidth = 1)+
  stat_bin(binwidth= 2) + ylim(c(0, 200)) +  
  stat_bin(binwidth= 2, geom="text", aes(label=..count..), vjust=-1.5) + 
  ggtitle("Percent Difference Distribution of Pharmacy and Health") + 
  theme(plot.title = element_text(color="blue", size=14, face="bold.italic"))
```

The histograms show that even with removal of significant outliers the data is still skewed with a large range of values.  Ideally the next step would be to perform a multi-factor analysis of variance (ANOVA) to determine if there are significance differences between the categories or a logistic regression to see which category correlates the best to percent difference.  I did not pursue these traditional statistical tests as the data was non-normal and may still contain data entry errors.  

Instead, I performed a simple analysis that analyzed the percentage of times the data showed better pricing on-line versus in-store.  To perform this analysis, I created a new column that created a value of 1 if the p_difference was 0 or greater and 0 if the value was less than zero.  Negative percent difference values show better prices on-line. I will then take the mean to determine what percentage of time the values showed better pricing in-store by category.  

First, I recombined my categories to form a single data set and then create a new column called bias, that assigns a value of 0 if the p_difference is less than zero and 1 if greater or equal to zero. 

```{r, message=FALSE}
# Recombind the various categories into one data frame
w_price3 <- rbind(electronics_set, home_app, pharm_health, mix, office)

w_price4 <- mutate(w_price3, bias = ifelse(p_difference  >= 0, 1, 0))

```

Then I calculated the mean by category.

```{r, echo=FALSE, message=FALSE}
# Create new sets to calculate the distribution
electronics_set <- filter(w_price4, category == "Electronics")
home_app <- filter(w_price4, category == "Home and Appliances")
mix <- filter(w_price4, category == "Mix")
office <- filter(w_price4, category =="Office Products")
pharm_health <- filter(w_price4,category == "Pharmacy and Health")

# Calculate the mean to determine percentage of time, it was neutral 
# or better to buy in store  

Electronics <-mean(electronics_set$bias)
Home <- mean(home_app$bias)
Mix<- mean(mix$bias)
Office <-mean(office$bias)

Electronics
Home
Mix 
Office
```

From the mean values you can see the percentage of time in-store has better or neutral pricing versus on-line.

* Electronics = 23.65% better pricing in-store
* Home Products = 51.02% better pricing in-store
* Mix Products = 53.04% better pricing in-store
* Office Products = 36.99% better pricing in-store

Thus, I would to my client it would be better to purchase electronics and office products on-line.  

#### Analyzing if sale price or regular price shows better value in-store or on-line:

The next analysis will determine what percentage of time the data showed better pricing in the store when the item was on sale or regular priced.  I inspected the variables, PRICETYPE and sale_online.  

PRICETYPE contains the values Regular Price and Sale/Discounted Price.  The variable also contains missing values.  The sale_online variable contains a value of 1 if the price is on sale and NA if the price is regular price.  Based on the values there are four conditions that item can have.

* Cat 1 = regular price online, regular price in store
* Cat 2 = sale online, regular price in store
* Cat 3 = regular price online, sale price in store
* Cat 4 = sale price online, sale price in store

I need to create a variable, "cat_price" that will evaluate the conditions in the PRICETYPE and sale_online variables then create a character value that specifies that condition. 

```{r, message=FALSE}
w_price4 %>% 
  mutate(cat_price = ifelse(PRICETYPE == 'Regular Price', 
                            ifelse(is.na(sale_online), 'Cat 1', 'Cat 2'), 
                            ifelse(PRICETYPE == 'Sale/Discounted Price', 
                                   ifelse(is.na(sale_online), 'Cat 3', 'Cat 4'), 
                                   NA))) -> w_price5
```

I include a statement to assign an NA if the PRICETYPE is missing.  After creating the new column, I graph the categories to see the various distributions.  

```{r, echo=FALSE, message=FALSE}
ggplot(w_price5, aes(x = cat_price, y = p_difference, col = cat_price)) + 
  geom_point()
```

There seems to be quite a bit of NA but at this point, I cannot make any logical assumptions with the data set so I decide to discard the NA.I pursue the same approach as before and split the data set by category type "cat_price" and calculate the bias mean by category type.  

```{r, echo=FALSE, message=FALSE}
cat1 <- filter(w_price5, cat_price =="Cat 1")
cat2 <- filter(w_price5, cat_price =="Cat 2")
cat3 <- filter(w_price5, cat_price =="Cat 3")
cat4 <- filter(w_price5, cat_price =="Cat 4")
```

The mean of the bias values tells me which Category provides the best pricing in the store.  

* Cat 1 = `r mean(cat1$bias)`
* Cat 2 = `r mean(cat2$bias)`
* Cat 3 = `r mean(cat3$bias)` 
* Cat 4 = `r mean(cat4$bias)`

The mean of the categories shows that only Category 3 provides better pricing in the store the majority of the time when the price is on sale in the store and regular price on-line.  

It is interesting that when the total data set was analyzed and the price bias was not broken up by categories, it was much better to buy items on-line versus in the store unless the item was only on sale in the store.  

Overall I would recommend my client purchase or evaluate purchasing items on-line to receive better price.  









******
[^1]: Cavallo, Alberto, 2016, “Are-Online and Offline Prices Similar? Evidence from Large Multi-Channel Retailers”- American Economic Review doi:10.7910/DVN/XXOUHF, Harvard Dataverse, V4 






 


  




